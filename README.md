# Awesome-AI-Security
A curated list of AI security resources

#### Legend:
|Type| Icon|
|---|---|
| Research  | ![Research](https://cdn4.iconfinder.com/data/icons/48-bubbles/48/12.File-32.png "Research")  |
| Slides  | ![Slides](https://cdn3.iconfinder.com/data/icons/tango-icon-library/48/x-office-presentation-32.png "Slides")  |
| Video | ![Video](https://cdn2.iconfinder.com/data/icons/snipicons/500/video-32.png "Video")  |
| Website / Blog post  | ![Blog post](https://cdn3.iconfinder.com/data/icons/tango-icon-library/48/internet-web-browser-32.png "Website or blog post")  |
| Code  | ![Code](https://cdn2.iconfinder.com/data/icons/snipicons/500/application-code-32.png "Code")  |

## Keywords:
- [Adversarial examples](#-adversarial-examples)
- Evasion attacks
- Poisoning attacks
- Feature selection
- Tutorials
- [Misc](#-misc)
- [Code](#-code)

## [▲](#keywords) Adversarial examples
|Type|Title|
|---|:---|
|![Research](https://cdn4.iconfinder.com/data/icons/48-bubbles/48/12.File-32.png "Research")  | [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572)  |
| ![Research](https://cdn4.iconfinder.com/data/icons/48-bubbles/48/12.File-32.png "Research")  | [Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples](https://arxiv.org/abs/1605.07277)  |
|![Research](https://cdn4.iconfinder.com/data/icons/48-bubbles/48/12.File-32.png "Research") |[On the (Statistical) Detection of Adversarial Examples](https://arxiv.org/abs/1702.06280)|
|![Research](https://cdn4.iconfinder.com/data/icons/48-bubbles/48/12.File-32.png "Research") |[The Space of Transferable Adversarial Examples](https://arxiv.org/abs/1704.03453)|
|![Research](https://cdn4.iconfinder.com/data/icons/48-bubbles/48/12.File-32.png "Research") |[Adversarial Attacks on Neural Network Policies](http://rll.berkeley.edu/adversarial/)|
|![Research](https://cdn4.iconfinder.com/data/icons/48-bubbles/48/12.File-32.png "Research") |[Adversarial Perturbations Against Deep Neural Networks for Malware Classification](https://arxiv.org/abs/1606.04435)|
|![Research](https://cdn4.iconfinder.com/data/icons/48-bubbles/48/12.File-32.png "Research") |[Crafting Adversarial Input Sequences for Recurrent Neural Networks](https://arxiv.org/abs/1604.08275)|
|![Research](https://cdn4.iconfinder.com/data/icons/48-bubbles/48/12.File-32.png "Research")|[Practical Black-Box Attacks against Machine Learning](https://arxiv.org/abs/1602.02697)|


## [▲](#keywords) Misc
|Type|Title|
|---|:---|
|![Research](https://cdn4.iconfinder.com/data/icons/48-bubbles/48/12.File-32.png "Research") |[Can Machine Learning Be Secure?](https://people.eecs.berkeley.edu/~adj/publications/paper-files/asiaccs06.pdf)|

## [▲](#keywords) Code
|Type|Title|
|---|:---|
|![Code](https://cdn2.iconfinder.com/data/icons/snipicons/500/application-code-32.png "Code") |[CleverHans - Python library to benchmark machine learning systems vulnerability to adversarial examples.](https://github.com/tensorflow/cleverhans)|
