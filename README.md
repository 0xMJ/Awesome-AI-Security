# Awesome AI Security ![](https://cdn3.iconfinder.com/data/icons/developperss/PNG/Star.png)
A curated list of AI security resources
    
[research]: https://cdn4.iconfinder.com/data/icons/48-bubbles/48/12.File-32.png "Research"
[slides]: https://cdn3.iconfinder.com/data/icons/tango-icon-library/48/x-office-presentation-32.png "Slides"
[video]: https://cdn2.iconfinder.com/data/icons/snipicons/500/video-32.png "Video"
[web]: https://cdn3.iconfinder.com/data/icons/tango-icon-library/48/internet-web-browser-32.png "Website or blog post"
[code]: https://cdn2.iconfinder.com/data/icons/snipicons/500/application-code-32.png "Code"
[other]: https://cdn3.iconfinder.com/data/icons/tango-icon-library/48/emblem-symbolic-link-32.png "Uncategorized"

#### Legend:
|Type| Icon|
|---|---|
| Research  | ![][research]|
| Slides  | ![][slides] |
| Video | ![][video]  |
| Website / Blog post  | ![][web]  |
| Code  | ![][code]|
| Other  | ![][other]|

## Keywords:
- [Adversarial examples](#-adversarial-examples)
- [Evasion attacks](#-evasion)
- [Poisoning attacks](#-poisoning)
- Feature selection
- Tutorials
- [Misc](#-misc)
- [Code](#-code)

## [▲](#keywords) Adversarial examples
|Type|Title|
|---|:---|
|![][research]  | [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572)  |
| ![][research]  | [Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples](https://arxiv.org/abs/1605.07277)  |
|![][research] |[On the (Statistical) Detection of Adversarial Examples](https://arxiv.org/abs/1702.06280)|
|![][research] |[The Space of Transferable Adversarial Examples](https://arxiv.org/abs/1704.03453)|
|![][research] |[Adversarial Attacks on Neural Network Policies](http://rll.berkeley.edu/adversarial/)|
|![][research] |[Adversarial Perturbations Against Deep Neural Networks for Malware Classification](https://arxiv.org/abs/1606.04435)|
|![][research] |[Crafting Adversarial Input Sequences for Recurrent Neural Networks](https://arxiv.org/abs/1604.08275)|
|![][research]|[Practical Black-Box Attacks against Machine Learning](https://arxiv.org/abs/1602.02697)|

## [▲](#keywords) Evasion
|Type|Title|
|---|:---|
|![][research]|[Query Strategies for Evading Convex-Inducing Classifiers](https://people.eecs.berkeley.edu/~adj/publications/paper-files/1007-0484v1.pdf)|
|![][research]|[Evasion attacks against machine learning at test time](https://pralab.diee.unica.it/sites/default/files/Biggio13-ecml.pdf)|

## [▲](#keywords) Poisoning
|Type|Title|
|---|:---|
|![][research] ![][slides]|[Poisoning Behavioral Malware Clustering](http://pralab.diee.unica.it/en/node/1121)|
|![][research]|[Efficient Label Contamination Attacks Against Black-Box Learning Models](https://www.ijcai.org/proceedings/2017/0551.pdf)|

## [▲](#keywords) Feature selection
|Type|Title|
|---|:---|
|![][research] ![][slides]|[Is Feature Selection Secure against Training Data Poisoning?](https://pralab.diee.unica.it/en/node/1191)|

## [▲](#keywords) Misc
|Type|Title|
|---|:---|
|![][research] |[Can Machine Learning Be Secure?](https://people.eecs.berkeley.edu/~adj/publications/paper-files/asiaccs06.pdf)|
|![][research]|[On The Integrity Of Deep Learning Systems In Adversarial Settings](https://etda.libraries.psu.edu/catalog/28680)|
## [▲](#keywords) Code
|Type|Title|
|---|:---|
|![][code]|[CleverHans - Python library to benchmark machine learning systems vulnerability to adversarial examples](https://github.com/tensorflow/cleverhans)|
